{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Init"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3032f5c8334051a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = datasets.MNIST(root='./data',train=True,download=True,transform=transforms)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T11:14:38.357607Z",
     "start_time": "2024-03-17T11:14:34.563602Z"
    }
   },
   "id": "be87d64a1c1ec9a4",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "118edbfff9494b94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b6d40980207c604"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=mnist_dataset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "train_data_iter = iter(train_loader)\n",
    "images = train_data_iter.__next__()\n",
    "num_sampler = 25\n",
    "sample_images = [images[0][i,0] for i in range(num_sampler)]\n",
    "sample_image = sample_images[0]\n",
    "image_size = sample_image.size()\n",
    "x_dim = image_size[0] * image_size[1]\n",
    "print(x_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:12:47.304258Z",
     "start_time": "2024-03-17T13:12:47.285255Z"
    }
   },
   "id": "ad7ecb91d4ac1238",
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": [
    "VAE Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768be480f3d2537f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 191\u001B[0m\n\u001B[0;32m    189\u001B[0m cvae_model \u001B[38;5;241m=\u001B[39m CVAE()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    190\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m Adam(cvae_model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m--> 191\u001B[0m \u001B[43mcvae_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcvae_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[80], line 176\u001B[0m, in \u001B[0;36mcvae_train\u001B[1;34m(model, optimizer, epochs, device)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;66;03m# target = target.type(torch.float32)\u001B[39;00m\n\u001B[0;32m    175\u001B[0m target \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 176\u001B[0m x_hat, mean, log_var, y_mean, y_log_var \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    177\u001B[0m loss \u001B[38;5;241m=\u001B[39m cvae_loss_function(data, x_hat, mean, log_var, y_mean, y_log_var)\n\u001B[0;32m    178\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "File \u001B[1;32mD:\\Programming\\Anaconda3\\envs\\aigc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Programming\\Anaconda3\\envs\\aigc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[80], line 158\u001B[0m, in \u001B[0;36mCVAE.forward\u001B[1;34m(self, x, y)\u001B[0m\n\u001B[0;32m    156\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameteriztion(mean, log_var)\n\u001B[0;32m    157\u001B[0m zy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameteriztion(y_mean, y_log_var)\n\u001B[1;32m--> 158\u001B[0m x_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mzy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_hat, mean, log_var, y_mean, y_log_var\n",
      "Cell \u001B[1;32mIn[80], line 139\u001B[0m, in \u001B[0;36mCVAE.decode\u001B[1;34m(self, x, y)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m,x, y):\n\u001B[1;32m--> 139\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(t)\n",
      "\u001B[1;31mTypeError\u001B[0m: cat() received an invalid combination of arguments - got (Tensor, Tensor), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "currentTime = time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime(time.time()))\n",
    "writer = SummaryWriter('./log/{currentTime}'.format(currentTime = currentTime))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim:int = 28 * 28, hidden_dim:int = 400, latent_dim:int = 200, device:str = 'cuda'):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # encoder产生z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim,latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        # 然后产生p(z|x)的均值和方差\n",
    "        # 图像是二维灰度图，所以方差和均值dim = 2\n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.log_var_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        # 均值和方差与z的维度一致\n",
    "        # self.mean_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        # self.log_var_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        # decoder根据p(x|z)产生新的采样\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def mean_log_var(self, x):\n",
    "        mean, log_var = self.mean_layer(x), self.log_var_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "    def reparameteriztion(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = self.mean_log_var(x)\n",
    "        z = self.reparameteriztion(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat,mean,log_var\n",
    "\n",
    "def vae_loss_function(x, x_hat, mean, log_var):\n",
    "    # 因为这个数据集的图形都是0/1，所以用交叉熵做Loss了\n",
    "    reproductive_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    # reproductive_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "    # reproductive_loss = nn.MSELoss(x_hat,x,reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproductive_loss + kld\n",
    "\n",
    "def vae_train(model,optimizer,epochs,device):\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ',epoch)\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.view(batch_size, x_dim).to(device)\n",
    "            x_hat,mean,log_var = model(data)\n",
    "            loss = vae_loss_function(data, x_hat, mean, log_var)\n",
    "            epoch_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('vae_loss/train', epoch_loss, epoch)\n",
    "        print(epoch_loss)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, class_dim:int = 10, input_dim: int = 28 * 28, hidden_dim: int = 400, latent_dim: int = 200, device: str = 'cuda'):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.class_dim = class_dim\n",
    "        \n",
    "        # embedding class\n",
    "        self.label_embedding = nn.Embedding(class_dim, latent_dim)\n",
    "        \n",
    "        # encoder产生z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        # 然后产生p(z|x, y)的均值和方差\n",
    "        # 图像是二维灰度图，所以方差和均值dim = 2\n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.log_var_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        self.y_mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.y_log_var_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        # 均值和方差与z的维度一致\n",
    "        # self.mean_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        # self.log_var_layer = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "        # decoder根据p(x|z, y)产生新的采样\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )   \n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def mean_log_var(self, x, y):\n",
    "        mean, log_var = self.mean_layer(x), self.log_var_layer(x)\n",
    "        y_mean, y_log_var = self.y_mean_layer(y), self.y_log_var_layer(y)\n",
    "        return mean, log_var, y_mean, y_log_var\n",
    "    \n",
    "    def reparameteriztion(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encode(x)\n",
    "        y = self.label_embedding(y)\n",
    "        mean, log_var = self.mean_log_var(x, y)\n",
    "        z = self.reparameteriztion(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "def cvae_loss_function(x, x_hat, mean, log_var):\n",
    "    # 因为这个数据集的图形都是0/1，所以用交叉熵做Loss了\n",
    "    reproductive_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproductive_loss + kld\n",
    "\n",
    "def cvae_train(model, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ', epoch)\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.view(batch_size, x_dim).to(device)\n",
    "            # target = target.type(torch.float32)\n",
    "            target = target.view(batch_size, 1).to(device)\n",
    "            x_hat, mean, log_var = model(data, target)\n",
    "            loss = cvae_loss_function(data, x_hat, mean, log_var)\n",
    "            epoch_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        writer.add_scalar('cvae_loss/train', epoch_loss, epoch)\n",
    "        print(epoch_loss)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# cvae train\n",
    "cvae_model = CVAE().to(device)\n",
    "optimizer = Adam(cvae_model.parameters(), lr=1e-3)\n",
    "cvae_train(cvae_model, optimizer, epochs=100, device=device)\n",
    "\n",
    "# # vae train\n",
    "# vae_model = VAE().to(device)\n",
    "# optimizer = Adam(vae_model.parameters(), lr = 1e-3)\n",
    "# vae_train(vae_model, optimizer, epochs=100, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:51:50.386893Z",
     "start_time": "2024-03-17T13:51:50.250866Z"
    }
   },
   "id": "abe0faf5c76c2423",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a52a953493f92068"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoints_path = './checkpoints/'\n",
    "checkpoint_name = f'{checkpoints_path}/vae_{currentTime}.pt'\n",
    "# torch.save(vae_model.state_dict(), checkpoint_name)\n",
    "\n",
    "checkpoint_name = f'{checkpoints_path}/cvae_{currentTime}.pt'\n",
    "torch.save(cvae_model.state_dict(), checkpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T12:25:37.217701Z",
     "start_time": "2024-03-17T12:25:37.197712Z"
    }
   },
   "id": "eb088e576f3af830",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18aa6b896b6aede"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vae_model_path = './checkpoints/vae_2024-03-17-19_14_38.pt'\n",
    "# vae_model = VAE().to(device)\n",
    "# vae_model.load_state_dict(torch.load(vae_model_path))\n",
    "# vae_model.eval()\n",
    "\n",
    "# cvae_model_path = './checkpoints/cvae_2024-03-17-19_14_38.pt'\n",
    "# cvae_model = CVAE().to(device)\n",
    "# cvae_model.load_state_dict(torch.load(cvae_model_path))\n",
    "# cvae_model.eval()\n",
    "\n",
    "def vae_generate_single_digit(mean, var):\n",
    "    with torch.no_grad():\n",
    "        z_sample = torch.tensor([[mean, var]],dtype=torch.float).to(device)\n",
    "        x_decoded = vae_model.decode(z_sample)\n",
    "        digit = x_decoded.detach().cpu().reshape(28, 28)\n",
    "        plt.imshow(digit, cmap='grey')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def cvae_generate_single_digit(y):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1,2, device = 'cuda')\n",
    "        z = cvae_model.condition_on_label(z, y)\n",
    "        x_decoded = cvae_model.decode(z)\n",
    "        digit = x_decoded.detach().cpu().reshape(28, 28)\n",
    "        plt.imshow(digit, cmap='grey')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "def plot_latent_space(model, scale=1, n=25, digit_size=28, fig_size=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # construct a grid \n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n",
    "            x_decoded = model.decode(z_sample)\n",
    "            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n",
    "\n",
    "    plt.figure(figsize=(fig_size, fig_size))\n",
    "    plt.title('VAE Latent Space Visualization')\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean, z [0]\")\n",
    "    plt.ylabel(\"var, z [1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cvae_generate_single_digit(torch.tensor([9],device='cuda',dtype=torch.float32))\n",
    "# plot_latent_space(vae_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:12:35.967449Z",
     "start_time": "2024-03-17T13:12:35.929441Z"
    }
   },
   "id": "1fb0375056ff92c",
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
